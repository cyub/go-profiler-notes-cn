liâ¬… [Index of all go-profiler-notes](../README.md)
# ä¸€ä»½ç»™å¿™ç¢Œå¼€å‘è€…çš„ Go Profilingã€Tracing å’Œ Observability æŒ‡å—

- **[å¯¼è®º](#introduction):** [é€‚åˆè¯»è€…](#read-this) Â· [Go çš„è®¤çŸ¥æ¨¡å‹](#mental-model-for-go) Â· Profiling vs Tracing
- **Use Cases:** Reduce Costs Â· Reduce Latency Â· Memory Leaks Â· Program Hanging Â· Outages
- **[Go åˆ†æå™¨](#go-profilers)**: [CPU åˆ†æå™¨](#cpu-profiler) Â· [å†…å­˜åˆ†æå™¨](#memory-profiler) Â· [é˜»å¡åˆ†æå™¨](#block-profiler) Â· [äº’æ–¥é”åˆ†æå™¨](#mutex-profiler) Â· [Goroutineåˆ†æå™¨](#goroutine-profiler) Â· [çº¿ç¨‹åˆ›å»ºåˆ†æå™¨](#threadcreate-profiler)
- **Viewing Profiles**: Command Line Â· Flame Graph Â· Graph
- **Go Execution Tracer:** Timeline View Â· Derive Profiles
- **Go Metrics:**  MemStats
- **Other Tools:** time Â· perf Â· bpftrace
- **[Advanced Topics](#advanced-topics):** Assembly Â· [Stack Traces](#stack-traces) Â· [pprof Format](#pprof-format) Â· Little's Law
- **Datadog Products:** Continuous Profiler Â· APM (Distributed Tracing) Â· Metrics

ğŸš§ This document is a work in progress. All sections above will become clickable links over time. The best way to find out about updates is to follow me and [my thread on twitter](https://twitter.com/felixge/status/1435537024388304900) where I'll announce new sections being added.

æœ¬æ–‡æ¡£æ­£åœ¨è¾“å‡ºä¸­ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä»¥ä¸Šæ‰€æœ‰éƒ¨åˆ†éƒ½å°†æˆä¸ºå¯ç‚¹å‡»çš„é“¾æ¥ã€‚äº†è§£æœ€æ–°çš„æœ€ä½³æ–¹å¼æ˜¯å…³æ³¨æˆ‘å’Œæˆ‘åœ¨ twitter ä¸Šçš„å¸–å­ï¼Œæˆ‘å°†åœ¨å…¶ä¸­å®£å¸ƒæ·»åŠ çš„æ–°éƒ¨åˆ†ã€‚

# å¯¼è®º

## é€‚åˆçš„è¯»è€…

This is a practical guide aimed at busy gophers interested in improving their programs using profiling, tracing and other observability techniques. If you're not well versed in the internals of Go, it is recommended that you read the entire introduction first. After that you should feel free to jump to any section you are interested in.

è¿™æ˜¯ä¸€æœ¬é¢å‘æœ‰å…´è¶£ä½¿ç”¨åˆ†æ(profiling)ã€è·Ÿè¸ª(tracing)å’Œå…¶ä»–å¯è§‚å¯Ÿæ€§æŠ€æœ¯(observability techniques)æ¥æ”¹è¿›ç¨‹åºçš„å¿™ç¢Œçš„Gophersçš„å®ç”¨æŒ‡å—ã€‚å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ Go çš„å†…éƒ¨ç»“æ„ï¼Œå»ºè®®æ‚¨å…ˆé˜…è¯»æ•´ä¸ªä»‹ç»ã€‚ä¹‹åï¼Œæ‚¨åº”è¯¥å¯ä»¥éšæ„è·³åˆ°æ‚¨ä»»æ„æ„Ÿå…´è¶£çš„éƒ¨åˆ†ã€‚

## Mental Model for Go
## Go çš„è®¤çŸ¥æ¨¡å‹

It is possible to become quite proficient in writing Go code without understanding how the language works under the hood. But when it comes to performance and debugging, you'll hugely benefit from having a mental model of the internals. Therefore we'll begin with laying out a rudimentary model of Go below. This model should be good enough to allow you to avoid the most common mistakes, but [all models are wrong](https://en.wikipedia.org/wiki/All_models_are_wrong), so you are encouraged to seek out more in-depth material to tackle harder problems in the future.

åœ¨ä¸äº†è§£ Go è¯­è¨€çš„åº•å±‚å·¥ä½œåŸç†çš„æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½ä¼šéå¸¸ç²¾é€šç¼–å†™ Go ç¨‹åºä»£ç ã€‚ä½†å½“æ¶‰åŠåˆ°æ€§èƒ½å’Œè°ƒè¯•é—®é¢˜æ—¶ï¼Œä½ å°†ä¼šä»Goå†…éƒ¨çš„è®¤çŸ¥æ¨¡å‹ä¸­å—ç›ŠåŒªæµ…ã€‚å› æ­¤æˆ‘ä»¬å°†é¦–å…ˆåœ¨ä¸‹é¢åˆ—å‡º Go çš„åŸºæœ¬æ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹åº”è¯¥è¶³å¤Ÿå¥½ï¼Œå¯ä»¥è®©ä½ é¿å…æœ€å¸¸è§çš„é”™è¯¯ï¼Œä½†[All models are wrong](https://en.wikipedia.org/wiki/All_models_are_wrong)ï¼Œå› æ­¤é¼“åŠ±ä½ å¯»æ‰¾æ›´æ·±å…¥çš„èµ„æ–™æ¥è§£å†³æœªæ¥æ›´éš¾çš„é—®é¢˜ã€‚

Go's primary job is to multiplex and abstract hardware resources, similar to an operating system. This is generally accomplished using two major abstractions:

Go çš„ä¸»è¦å·¥ä½œæ˜¯å¤ç”¨(multiplex)å’ŒæŠ½è±¡ç¡¬ä»¶èµ„æº(abstract hardware resources)ï¼Œå®ƒç±»ä¼¼äºæ“ä½œç³»ç»Ÿã€‚Go é€šå¸¸ä½¿ç”¨ä¸‹é¢ä¸¤ä¸ªä¸»è¦æŠ½è±¡æ¥å®Œæˆï¼š

1. **Goroutine Scheduler:** Manages how your code is being executed on the CPUs of your system.

  **Goroutine è°ƒåº¦å™¨** ç”¨äºæ§åˆ¶ä»£ç å¦‚ä½•åœ¨ç³»ç»Ÿ CPU ä¸Šçš„æ‰§è¡Œ
2. **Garbage Collector:** Provides virtual memory that is automatically freed as needed.
  **åƒåœ¾å›æ”¶å™¨** æä¾›å¯ä»¥æ ¹æ®éœ€è¦è‡ªåŠ¨é‡Šæ”¾çš„è™šæ‹Ÿå†…å­˜


### Goroutine Scheduler

### Goroutine è°ƒåº¦å™¨

Let's talk about the scheduler first using the example below:

æˆ‘ä»¬ä½¿ç”¨ä¸‹é¢è¿™ä¸ªä¾‹å­æ¥è®¨è®ºè°ƒåº¦å™¨ï¼š

```go
func main() {
    res, err := http.Get("https://example.org/")
    if err != nil {
        panic(err)
    }
    fmt.Printf("%d\n", res.StatusCode)
}
```

Here we have a single goroutine, let's call it `G1`, that runs the `main` function. The picture below shows a simplified timeline of how this goroutine might execute on a single CPU. Initially `G1` is running on the CPU to prepare the http request. Then the CPU becomes idle as the goroutine has to wait for the network. And finally it gets scheduled onto the CPU again to print out the status code.

è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå•ç‹¬çš„ goroutineï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º G1ï¼Œå®ƒè¿è¡Œ main å‡½æ•°ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†è¿™ä¸ª goroutine å¦‚ä½•åœ¨å•ä¸ª CPU ä¸Šæ‰§è¡Œçš„ç®€åŒ–ç‰ˆæ—¶é—´çº¿ã€‚æœ€åˆ G1 åœ¨ CPU ä¸Šè¿è¡Œä»¥å‡†å¤‡ http è¯·æ±‚ã€‚ç„¶å CPU å˜å¾—ç©ºé—²ï¼Œå› ä¸º goroutine å¿…é¡»ç­‰å¾…ç½‘ç»œã€‚æœ€åï¼Œå®ƒå†æ¬¡è¢«è°ƒåº¦åˆ° CPU ä¸Šä»¥æ‰“å°å‡ºçŠ¶æ€ç ã€‚

<img src="./timeline.png" width=600/>

From the scheduler's perspective, the program above executes like shown below. At first `G1` is `Executing` on `CPU 1`. Then the goroutine is taken off the CPU while `Waiting` for the network. Once the scheduler notices that the network has replied (using non-blocking I/O, similar to Node.js), it marks the goroutine as `Runnable`. And as soon as a CPU core becomes available, the goroutine starts `Executing` again. In our case all cores are available, so `G1` can go back to `Executing` the `fmt.Printf()` function on one of the CPUs immediately without spending any time in the `Runnable` state.

ä»è°ƒåº¦å™¨çš„è§’åº¦æ¥çœ‹ï¼Œä¸Šé¢çš„ç¨‹åºæ‰§è¡Œå¦‚ä¸‹æ‰€ç¤ºã€‚ä¸€å¼€å§‹ G1 åœ¨ CPU 1 ä¸Šæ‰§è¡Œã€‚æ¥ç€ goroutine åœ¨ç”±äºç­‰å¾…ç½‘ç»œæ—¶ä¼šä» CPU ä¸­taken offã€‚ä¸€æ—¦è°ƒåº¦å™¨æ³¨æ„åˆ°ç½‘ç»œå·²ç»å“åº”ï¼ˆä½¿ç”¨éé˜»å¡ I/Oï¼Œç±»ä¼¼äº Node.jsï¼‰ï¼Œå®ƒä¼šå°† goroutine æ ‡è®°ä¸º `Runnable`ã€‚ä¸€æ—¦ CPU å†…æ ¸å¯ç”¨ï¼Œgoroutine å°±ä¼šå†æ¬¡å¼€å§‹æ‰§è¡Œã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæ‰€æœ‰å†…æ ¸éƒ½å¤„äºå¯ç”¨çŠ¶æ€ï¼Œå› æ­¤ G1 å¯ä»¥ç«‹å³è¿”å›åˆ°åœ¨å…¶ä¸­ä¸€ä¸ª CPU ä¸Šæ‰§è¡Œ fmt.Printf() å‡½æ•°ï¼Œè€Œæ— éœ€èŠ±è´¹ä»»ä½•æ—¶é—´å¤„äº `Runnable` çŠ¶æ€ã€‚


<img src="./scheduler.gif" width=400/>

Most of the time, Go programs are running multiple goroutines, so you will have a few goroutines `Executing` on some of the CPU cores, a large number of goroutines `Waiting` for various reasons, and ideally no goroutines `Runnable` unless your program exhibits very high CPU load. An example of this can be seen below.

å¤§å¤šæ•°æ—¶å€™ï¼ŒGo ç¨‹åºéƒ½åœ¨è¿è¡Œå¤šä¸ª goroutineï¼Œæ‰€ä»¥ä¼šæœ‰å‡ ä¸ª goroutine æ­£åœ¨æ‰§è¡Œåœ¨ CPU å†…æ ¸ä¸Šï¼Œæœ‰å¤§é‡ goroutine ç”±äºå„ç§åŸå› å¤„äº`Waiting`ï¼Œåœ¨ç†æƒ³æƒ…å†µä¸‹æ²¡æœ‰ goroutine å¤„äº`Ruannable` çŠ¶æ€ï¼Œé™¤éä½ çš„ç¨‹åºè¡¨ç°å‡ºéå¸¸é«˜çš„ CPU è´Ÿè½½ã€‚è¿™æ–¹é¢çš„ä¸€ä¸ªä¾‹å­å¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°ã€‚

<img src="./scheduler-complete.png" width=600/>

Of course the model above glosses over many details. In reality it's turtles all the way down, and the Go scheduler works on top of threads managed by the operating system, and even CPUs themselves are capable of hyper-threading which can be seen as a form of scheduling. So if you're interested, feel free to continue down this rabbit hole via Ardan labs series on [Scheduling in Go](https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html) or similar material.

å½“ç„¶ï¼Œä¸Šé¢çš„æ¨¡å‹æ©ç›–äº†è®¸å¤šç»†èŠ‚ã€‚å®é™…ä¸Šï¼Œä¸€è·¯å‘ä¸‹æ¢ç©¶(it's turtles all the way down)ï¼Œä½ ä¼šå‘ç°Go è°ƒåº¦ç¨‹åºå·¥ä½œåœ¨æ“ä½œç³»ç»Ÿç®¡ç†çš„çº¿ç¨‹ä¹‹ä¸Šï¼Œç”šè‡³ CPU æœ¬èº«ä¹Ÿèƒ½å¤Ÿè¿›è¡Œè¶…çº¿ç¨‹å¤„ç†ï¼Œè¿™å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§è°ƒåº¦å½¢å¼ã€‚å¦‚æœæ‚¨æœ‰å¯¹æ­¤æ„Ÿå…´è¶£çš„è¯ï¼Œå¯ä»¥å»é˜…è¯» Ardan å®éªŒå®¤[å…³äº Go è°ƒåº¦](https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html)æ–‡ç« ç³»åˆ—æˆ–ç±»ä¼¼ææ–™ã€‚

However, the model above should be sufficient to understand the remainder of this guide. In particular it should become clear that the time measured by the various Go profilers is essentially the time your goroutines are spending in the `Executing` and `Waiting` states as illustrated by the diagram below.

ä¸ç®¡æ€ä¹ˆè¯´ï¼Œä¸Šé¢çš„æ¨¡å‹åº”è¯¥è¶³ä»¥ç†è§£æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†ã€‚æˆ‘ä»¬å°¤å…¶åº”è¯¥æ¸…æ¥šçš„æ˜¯ï¼Œå„ç§ Go åˆ†æå™¨æµ‹é‡çš„æ—¶é—´æœ¬è´¨ä¸Šæ˜¯æ‚¨çš„ goroutine åœ¨æ‰§è¡Œå’Œç­‰å¾…çŠ¶æ€ä¸­èŠ±è´¹çš„æ—¶é—´ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

<img src="./profiler-venn.png" width=800/>

### Garbage Collector

### åƒåœ¾å›æ”¶å™¨

The other major abstraction in Go is the garbage collector. In languages like C, the programmer needs to manually deal with allocating and releasing memory using `malloc()` and `free()`. This offers great control, but turns out to be very error prone in practice. A garbage collector can reduce this burden, but the automatic management of memory can easily become a performance bottleneck. This section of the guide will present a simple model for Go's GC that should be useful for identifying and optimizing memory management related problems.

Go ä¸­çš„å¦ä¸€ä¸ªä¸»è¦æŠ½è±¡æ˜¯åƒåœ¾å›æ”¶å™¨ã€‚åƒ C è¿™æ ·çš„è¯­è¨€ï¼Œç¨‹åºå‘˜éœ€è¦ä½¿ç”¨ `malloc()` å’Œ `free()` æ‰‹åŠ¨å¤„ç†å†…å­˜çš„åˆ†é…å’Œé‡Šæ”¾ã€‚è¿™æä¾›äº†å¾ˆå¥½çš„æ§åˆ¶ï¼Œä½†åœ¨å®è·µä¸­å¾ˆå®¹æ˜“å‡ºé”™ã€‚åƒåœ¾å›æ”¶å™¨å¯ä»¥å‡è½»è¿™ç§è´Ÿæ‹…ï¼Œä½†å†…å­˜çš„è‡ªåŠ¨ç®¡ç†å¾ˆå®¹æ˜“æˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚æœ¬æŒ‡å—çš„è¿™ä¸€éƒ¨åˆ†å°†å±•ç¤ºä¸€ä¸ªç®€å•çš„ Go GC æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹äºè¯†åˆ«å’Œä¼˜åŒ–å†…å­˜ç®¡ç†ç›¸å…³é—®é¢˜åº”è¯¥å¾ˆæœ‰ç”¨ã€‚

#### The Stack

#### æ ˆ

Let's start with the basics. Go can allocate memory in one of two places, the stack or the heap. Each goroutine has its own stack which is a contiguous area of memory. Additionally there is a big area of memory shared between goroutines that is called the heap. This can be seen in the picture below.

è®©æˆ‘ä»¬ä»åŸºç¡€å¼€å§‹ã€‚ Go å¯ä»¥åœ¨å †æ ˆ(stack)æˆ–å †(heap)è¿™ä¸¤ä¸ªåœ°æ–¹ä¹‹ä¸€åˆ†é…å†…å­˜ã€‚æ¯ä¸ª goroutine éƒ½æœ‰è‡ªå·±çš„å †æ ˆï¼Œè¿™æ˜¯ä¸€ä¸ªè¿ç»­çš„å†…å­˜åŒºåŸŸã€‚æ­¤å¤–ï¼Œåœ¨ goroutine ä¹‹é—´å…±äº«çš„ä¸€å¤§å—å†…å­˜åŒºåŸŸç§°ä¸ºå †ã€‚Goçš„å †æ ˆå’Œå †å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<img src="./heap-simple.png" width=650/>

When a function calls another function it gets its own section on the stack called a stack frame where it can place things like local variables. A stack pointer is used to identify the next free spot in the frame. When a function returns, the data from the last frame is discarded by simply moving the stack pointer back to end of the previous frame. The frame's data itself can linger on the stack, and gets overwritten by the next function call. This is very simple and efficient as Go doesn't have to keep track of each variable.

å½“ä¸€ä¸ªå‡½æ•°è°ƒç”¨å¦ä¸€ä¸ªå‡½æ•°æ—¶å€™ï¼Œå®ƒå°†ä¼šä»æ ˆä¸Šè·å¾—è‡ªå·±çš„ç©ºé—´ï¼Œè¿™éƒ¨åˆ†ç©ºé—´å«åšæ ˆå¸§(stack frame)ï¼Œæ ˆå¸§æ˜¯ç”¨æ¥å­˜å±€éƒ¨å˜é‡ç­‰å†…å®¹ã€‚æ ˆæŒ‡é’ˆç”¨äºæ ‡è¯†å¸§ä¸­çš„ä¸‹ä¸€ä¸ªç©ºé—²ç‚¹ã€‚å½“å‡½æ•°è¿”å›æ—¶ï¼Œåªéœ€å°†å †æ ˆæŒ‡é’ˆç§»å›å‰ä¸€å¸§çš„æœ«å°¾å³å¯ä¸¢å¼ƒæœ€åä¸€å¸§çš„æ•°æ®ã€‚æ ˆå¸§ä¸Šçš„æ•°æ®æœ¬èº«å¯ä»¥åœ¨æ ˆä¸Šå­˜æ”¾ï¼Œå¹¶è¢«ä¸‹ä¸€ä¸ªå‡½æ•°è°ƒç”¨è¦†ç›–ã€‚è¿™æ˜¯éå¸¸ç®€å•å’Œé«˜æ•ˆçš„ï¼Œå› ä¸º Go ä¸å¿…è·Ÿè¸ªæ¯ä¸ªå˜é‡ã€‚


To make this a little more intuitive, let's look at the example below:
ä¸ºäº†ä¸Šé¢è®¨è®ºæ›´ç›´è§‚ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸‹é¢çš„ä¾‹å­ï¼š

```go
func main() {
	sum := 0
	sum = add(23, 42)
	fmt.Println(sum)
}

func add(a, b int) int {
	return a + b
}
```

Here we have a `main()` function that starts out by reserving some space on the stack for the variable `sum`. When the `add()` function gets called, it gets its own frame to hold the local `a` and `b` parameters. Once the `add()` returns, its data is discarded by moving the stack pointer back to the end of the `main()` function's frame, and the `sum` variable gets updated with the result. Meanwhile the old values of `add()` linger beyond the stack pointer to be overwritten by the next function call. Below is a visualization of this process:

è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ª `main()` å‡½æ•°ï¼Œå®ƒé¦–å…ˆåœ¨æ ˆä¸Šä¸ºå˜é‡ `sum` ä¿ç•™ä¸€äº›ç©ºé—´ã€‚å½“ `add()` å‡½æ•°è¢«è°ƒç”¨æ—¶ï¼Œå®ƒä¼šä½¿ç”¨è‡ªå·±çš„æ ˆå¸§ç©ºé—´æ¥ä¿å­˜æœ¬åœ° `a` å’Œ `b` å‚æ•°ã€‚ä¸€æ—¦ `add()` è¿”å›ï¼Œå®ƒçš„æ•°æ®é€šè¿‡å°†æ ˆæŒ‡é’ˆç§»å›åˆ° `main()` å‡½æ•°å¸§çš„æœ«å°¾è€Œè¢«ä¸¢å¼ƒï¼Œå¹¶ä¸” `sum` å˜é‡è¢«æ›´ç»†ä¸ºå‡½æ•°`add()`çš„è¿”å›å€¼ã€‚åŒæ—¶ `add()` çš„æ—§å€¼åœ¨å †æ ˆæŒ‡é’ˆä¹‹å¤–å¾˜å¾Šï¼Œå°†è¢«ä¸‹ä¸€ä¸ªå‡½æ•°è°ƒç”¨è¦†ç›–ã€‚ä¸‹é¢æ˜¯è¿™ä¸ªè¿‡ç¨‹çš„å¯è§†åŒ–ï¼š

<img src="./stack.gif" width=400/>

The example above is highly simplified and omits many details around return values, frame pointers, return addresses and function inlining. In fact, as of Go 1.17, the program above may not even need any space on the stack as the small amount of data can be managed using CPU registers by the compiler. But that's okay. This model should still give you a reasonable intuition for the way non-trivial Go programs allocate and discard local variables on the stack.

ä¸Šé¢çš„ä¾‹å­æ˜¯é«˜åº¦ç®€åŒ–çš„ï¼Œçœç•¥äº†å¾ˆå¤šå…³äºè¿”å›å€¼ã€å¸§æŒ‡é’ˆã€è¿”å›åœ°å€å’Œå‡½æ•°å†…è”çš„ç»†èŠ‚ã€‚äº‹å®ä¸Šï¼Œä» Go 1.17 å¼€å§‹ï¼Œä¸Šé¢çš„ç¨‹åºç”šè‡³å¯èƒ½ä¸éœ€è¦å †æ ˆä¸Šçš„ä»»ä½•ç©ºé—´ï¼Œå› ä¸ºç¼–è¯‘å™¨å¯ä»¥ä½¿ç”¨ CPU å¯„å­˜å™¨ç®¡ç†å°‘é‡æ•°æ®ã€‚ä½†æ˜¯æ²¡å…³ç³»ã€‚è¿™ä¸ªæ¨¡å‹ä»ç„¶èƒ½å¤Ÿè®©ä½ å¯¹éå‡¡çš„ Go ç¨‹åºåœ¨å †æ ˆä¸Šåˆ†é…å’Œä¸¢å¼ƒå±€éƒ¨å˜é‡çš„æ–¹å¼æœ‰ä¸€ä¸ªç›´è§‚æ„Ÿå—ã€‚

One thing you might wonder at this point is what happens if you run out of space on the stack. In languages like C this would cause a stack overflow error. Go on the other hand automatically deals with this problem by making a copy of the stack that's twice as big. This allows goroutines to start out with very small, typically 2 KiB stacks, and is one of the key ingredients for [making goroutines more scalable](https://golang.org/doc/faq#goroutines) than operating system threads.

ä½ å¯èƒ½æƒ³çŸ¥é“å¦‚æœæ ˆç©ºé—´ä¸è¶³æ—¶å€™å°†ä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿåœ¨ C è¿™æ ·çš„è¯­è¨€ä¸­ï¼Œå®ƒå°†ä¼šå¯¼è‡´æ ˆæº¢å‡ºé”™è¯¯ï¼ˆstack overflow errorï¼‰ã€‚ç„¶å Go æ˜¯é€šè¿‡å¤åˆ¶å‡ºä¸¤å€å¤§çš„å †æ ˆæ¥è‡ªåŠ¨å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œè¿™ç§æ–¹å¼å…è®¸ goroutines å¯ä»¥ä»å¾ˆå°çš„æ ˆç©ºé—´å¼€å§‹ï¼Œé€šå¸¸ä¸º 2 KiBï¼Œä¹Ÿæ˜¯ä½¿ [goroutines æ¯”æ“ä½œç³»ç»Ÿçº¿ç¨‹æ›´å…·å¯æ‰©å±•æ€§](https://golang.org/doc/faq#goroutines)çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚

Another aspect of the stack is how it's involved in creating stack traces. This is a bit more advanced, but if you're interested check out the [Stack Traces in Go](../stack-traces.md) document in this repo.

æ ˆçš„å¦ä¸€ä¸ªæ–¹é¢æ˜¯å®ƒå¦‚ä½•å‚ä¸åˆ›å»ºå †æ ˆè·Ÿè¸ªã€‚è¿™è¯é¢˜æœ‰ç‚¹å¤æ‚ï¼Œä½†å¦‚æœæ‚¨æœ‰å…´è¶£ï¼Œè¯·æŸ¥çœ‹æ­¤ä»“åº“ä¸­çš„å…³äº [Go æ ˆè·Ÿè¸ª](https://github.com/DataDog/go-profiler-notes/blob/main/stack-traces.md)éƒ¨åˆ†æ–‡æ¡£ã€‚

#### The Heap

#### å †

Stack allocations are great, but there are many situations where Go can't utilize them. The most common one is returning a pointer to a local variable of a function. This can be seen in this modified version of our `add()` example from above:

æ ˆåˆ†é…å¾ˆå¥½ï¼Œä½†åœ¨å¾ˆå¤šæƒ…å†µä¸‹ Go æ— æ³•ä½¿ç”¨å®ƒä»¬ã€‚æœ€å¸¸è§çš„æ˜¯è¿”å›æŒ‡å‘å‡½æ•°å±€éƒ¨å˜é‡çš„æŒ‡é’ˆã€‚è¿™å¯ä»¥åœ¨ä¸Šé¢çš„ add() ç¤ºä¾‹çš„ä¿®æ”¹ç‰ˆæœ¬ä¸­çœ‹åˆ°ï¼š

```go
func main() {
	fmt.Println(*add(23, 42))
}

func add(a, b int) *int {
	sum := a + b
	return &sum
}
```

Normally Go would be able to allocate the `sum` variable inside of the `add()` function on the stack. But as we've learned, this data gets discarded when the `add()` function returns. So in order to safely return a `&sum` pointer, Go has to allocate the memory for it from outside of the stack. And that's where the heap comes in.

é€šå¸¸ Go å°†èƒ½å¤Ÿåœ¨æ ˆä¸Šä¸º `add()` å‡½æ•°å†…éƒ¨çš„ `sum` å˜é‡åˆ†é…ç©ºé—´ã€‚ä½†æ­£å¦‚æˆ‘ä»¬ä¸Šé¢ä»‹ç»çš„é‚£æ ·ï¼Œå½“ `add()` å‡½æ•°è¿”å›æ—¶ï¼Œè¿™äº›æ•°æ®ä¼šè¢«ä¸¢å¼ƒã€‚å› æ­¤ï¼Œä¸ºäº†å®‰å…¨åœ°è¿”å› `&sum` æŒ‡é’ˆï¼ŒGo å¿…é¡»ä»æ ˆå¤–éƒ¨ä¸ºå…¶åˆ†é…å†…å­˜ã€‚è¿™å°±æ˜¯å †çš„ç”¨æ­¦ä¹‹åœ°ã€‚

The heap is used for storing data that might outlive the function that creates it, as well as for any data that is shared between goroutines using pointers. However, this raises the question of how this memory gets freed. Because unlike stack allocations, heap allocations can't be discarded when the function that created them returns.

å †ç”¨äºå­˜å‚¨å¯èƒ½æ¯”åˆ›å»ºå®ƒçš„å‡½æ•°å£°æ˜å‘¨æœŸæ›´é•¿çš„æ•°æ®ï¼Œä»¥åŠä½¿ç”¨æŒ‡é’ˆåœ¨ goroutine ä¹‹é—´å…±äº«çš„ä»»ä½•æ•°æ®ã€‚ç„¶è€Œè¿™å°±æ¶‰åŠäº†å¦‚ä½•é‡Šæ”¾è¿™äº›å†…å­˜çš„é—®é¢˜ã€‚å› ä¸ºä¸æ ˆåˆ†é…ä¸åŒï¼Œå †åˆ†é…åœ¨åˆ›å»ºå®ƒä»¬çš„å‡½æ•°è¿”å›æ—¶ä¸èƒ½è¢«ä¸¢å¼ƒ(discard)ã€‚

Go solves this problem using its built-in garbage collector. The details of its implementation are very complex, but from a birds eye view, it keeps track of your memory as shown in the picture below. Here you can see three goroutines that have pointers to green allocations on the heap. Some of these allocations also have pointers to other allocations shown in green. Additionally there are grey allocations that may point to the green allocations or each other, but they are not referenced by a green allocation themselves. Those allocations were once reachable, but are now considered to be garbage. This can happen if the function that allocated their pointers on the stack returned, or their value was overwritten. The GC is responsible for automatically identifying and freeing those allocations.

Go ä½¿ç”¨å…¶å†…ç½®çš„åƒåœ¾å›æ”¶å™¨(built-in garbage collector)è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚å®ƒçš„å®ç°ç»†èŠ‚éå¸¸å¤æ‚ï¼Œä½†ä»ä¿¯ç°çš„è§’åº¦æ¥çœ‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå®ƒä¼šè·Ÿè¸ªåº”ç”¨ç¨‹åºçš„å†…å­˜ã€‚ä¸‹å›¾ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä¸‰ä¸ª goroutineï¼Œä»–ä»¬éƒ½æœ‰æŒ‡é’ˆæŒ‡å‘å †ä¸Šçš„ç»¿è‰²åŒºåŸŸï¼Œå…¶ä¸­çš„ä¸€äº›åŒºåŸŸè¿˜ä¼šæŒ‡å‘å…¶ä»–çš„ç»¿è‰²åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¿˜æœ‰æŒ‡å‘ç»¿è‰²åŒºåŸŸçš„ç°è‰²åŒºåŸŸï¼Œæˆ–è€…ç›¸äº’æŒ‡å‘çš„ç°è‰²åŒºåŸŸï¼Œä½†å®ƒä»¬ä¸è¢«ç»¿è‰²åŒºåŸŸæœ¬èº«å¼•ç”¨ã€‚è¿™äº›ç°è‰²æ›¾ç»æ˜¯å¯ä»¥è®¿é—®çš„ï¼Œä½†ç°åœ¨è¢«è®¤ä¸ºæ˜¯åƒåœ¾ã€‚å¦‚æœåœ¨å †æ ˆä¸Šåˆ†é…æŒ‡é’ˆçš„å‡½æ•°è¿”å›ï¼Œæˆ–è€…å®ƒä»¬çš„å€¼è¢«è¦†ç›–ï¼Œå°±ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚ GC è´Ÿè´£è‡ªåŠ¨è¯†åˆ«å’Œé‡Šæ”¾è¿™äº›åŒºåŸŸã€‚

<img src="./heap-gc.gif" width=650/>

Performing GC involves a lot of expensive graph traversal and cache thrashing. It even requires regular stop-the-world phases that halt the execution of your entire program. Luckily recent versions of Go have gotten this down to fractions of a millisecond, but much of the remaining overhead is inherent to any GC. In fact, it's not uncommon that 20-30% of a Go program's execution are spend on memory management.

æ‰§è¡Œ GC æ“ä½œæ¶‰åŠå¤§é‡æ€§èƒ½æŸè€—åœ¨çš„å›¾éå†(graph traversal)å’Œç¼“å­˜æŠ–åŠ¨(cache thrashing)ã€‚å®ƒç”šè‡³éœ€è¦å®šæœŸåœæ­¢æ•´ä¸ªç¨‹åºçš„æ‰§è¡Œé˜¶æ®µã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ€è¿‘çš„ Go ç‰ˆæœ¬å·²å°†å…¶é™ä½åˆ°å‡ åˆ†ä¹‹ä¸€æ¯«ç§’ï¼Œä½†å¤§éƒ¨åˆ†å‰©ä½™å¼€é”€æ˜¯ä»»ä½• GC æ‰€å›ºæœ‰çš„ã€‚äº‹å®ä¸Šï¼ŒGo ç¨‹åºä¸­ 20-30% çš„æ‰§è¡ŒèŠ±è´¹åœ¨å†…å­˜ç®¡ç†ä¸Šçš„æƒ…å†µå¹¶ä¸å°‘è§ã€‚

Generally speaking the cost of GC is proportional to the amount of heap allocations your program performs. So when it comes to optimizing the memory related overhead of your program, the mantra is:

ä¸€èˆ¬æ¥è¯´ï¼ŒGC çš„æˆæœ¬ä¸ç¨‹åºæ‰§è¡Œçš„å †åˆ†é…é‡æˆæ­£æ¯”ã€‚å› æ­¤ï¼Œåœ¨ä¼˜åŒ–ç¨‹åºçš„å†…å­˜ç›¸å…³å¼€é”€æ—¶ï¼Œå£å¤´ç¦…æ˜¯ï¼š


- **Reduce**: Try to turn heap allocations into stack allocations or avoid them altogether. Minimizing the number of pointers on the heap also helps.
- **å‡å°‘ä½¿ç”¨ï¼š** å°è¯•å°†å †åˆ†é…è½¬æ¢ä¸ºæ ˆåˆ†é…æˆ–å®Œå…¨é¿å…å®ƒä»¬ã€‚æœ€å°åŒ–å †ä¸Šçš„æŒ‡é’ˆæ•°é‡ä¹Ÿä¼šæœ‰æ‰€å¸®åŠ©ã€‚

- **Reuse:** Reuse heap allocations rather than replacing them with new ones.
- **å¤ç”¨ï¼š** å¤ç”¨åˆ†é…çš„å †è€Œä¸æ˜¯ä½¿ç”¨æ–°çš„æ¥æ›¿æ¢å®ƒä»¬ã€‚

- **Recycle:** Some heap allocations can't be avoided. Let the GC recycle them and focus on other issues.
- **å›æ”¶ï¼š** ä¸€äº›å †åˆ†é…æ˜¯æ— æ³•é¿å…çš„ã€‚è®© GC å›æ”¶å®ƒä»¬å¹¶ä¸“æ³¨äºå…¶ä»–é—®é¢˜ã€‚

As with the previous mental model in this guide, everything above is an extremely simplified view of reality. But hopefully it will be good enough to make sense out of the remainder of this guide, and inspire you to read more articles on the subject. One article you should definitely read is [Getting to Go: The Journey of Go's Garbage Collector](https://go.dev/blog/ismmkeynote) which gives you a good idea of how Go's GC has advanced over the years, and the pace at which it is improving.

ä¸æœ¬æŒ‡å—ä¸­ä¹‹å‰çš„è®¤çŸ¥æ¨¡å‹ä¸€æ ·ï¼Œä»¥ä¸Šæ‰€æœ‰å†…å®¹éƒ½æ˜¯å¯¹ç°å®çš„æå…¶ç®€åŒ–çš„æ¦‚è§ˆï¼Œä½†å¸Œæœ›å®ƒè¶³ä»¥è®©æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†å˜å¾—æœ‰æ„ä¹‰ï¼Œå¹¶æ¿€åŠ±æ‚¨é˜…è¯»æ›´å¤šå…³äºè¯¥ä¸»é¢˜çš„æ–‡ç« ã€‚æ‚¨ç»å¯¹åº”è¯¥é˜…è¯»çš„ä¸€ç¯‡æ–‡ç« æ˜¯ [Getting to Go: The Journey of Go's Garbage Collector](https://go.dev/blog/ismmkeynote)ï¼Œå®ƒè®©æ‚¨å¾ˆå¥½åœ°äº†è§£ Go çš„ GC å¤šå¹´æ¥æ˜¯å¦‚ä½•å‘å±•çš„ï¼Œä»¥åŠå®ƒçš„æ”¹è¿›é€Ÿåº¦ã€‚

# Go Profilers

# Go åˆ†æå™¨

Here is an overview of the profilers built into the Go runtime. For more details following the links.

ä¸‹é¢æ˜¯ Go è¿è¡Œæ—¶ä¸­å†…ç½®çš„åˆ†æå™¨çš„æ¦‚è¿°ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®åé¢çš„é“¾æ¥ã€‚

| | [CPU](#cpu-profiler) | [å†…å­˜(Memory)](#memory-profiler) | [é˜»å¡(Block)]](#block-profiler) | [äº’æ–¥é”(Mutex)](#mutex-profiler) | [Goroutine](#goroutine-profiler) | [çº¿ç¨‹åˆ›å»º(ThreadCreate)](#threadcreate-profiler) |
|-|-|-|-|-|-|-|
|ç”Ÿäº§ç¯å¢ƒä½¿ç”¨å®‰å…¨æ€§(Production Safety)|âœ…|âœ…|âš  (1.)|âœ…|âš ï¸ (2.)|ğŸ (3.)|
|å®‰å…¨ç‡(Safe Rate)|default|default|âŒ (1.)|`100`|`1000` goroutines|-|
|å‡†ç¡®æ€§(Accuracy)|â­ï¸â­|â­â­â­|â­â­â­|â­â­â­|â­â­â­|-|
|æœ€å¤§æ ˆæ·±åº¦(Max Stack Depth)|`64`|`32`|`32`|`32`|`32` - `100` (4.)|-|
|åˆ†æå™¨æ ‡ç­¾æ”¯æŒ(Profiler Labels)|âœ…|âŒ|âŒ|âŒ|âœ…|-|

1. The block profiler can be a significant source of CPU overhead if configured incorrectly. See the [warning](#block-profiler-limitations).
  å¦‚æœé…ç½®ä¸æ­£ç¡®ï¼Œé˜»å¡åˆ†æå™¨(block profiler)å¯èƒ½æ˜¯ CPU å¼€é”€çš„é‡è¦æ¥æºã€‚è¯¦æƒ…[è§è­¦å‘Š]ã€‚(#block-profiler-limitations)ã€‚
2. One O(N) stop-the-world pauses where N is the number of goroutines. Expect ~1-10Âµsec pause per goroutine.
  O(N) çš„stop-the-world æš‚åœï¼ŒNæ˜¯goroutinesçš„æ•°é‡ï¼Œæ¯ä¸ªgoroutineæš‚åœè€—æ—¶~1-10Âµsecã€‚
3. Totally broken, don't try to use it.
  ä¸è¦å°è¯•ä½¿ç”¨ã€‚

4. Depends on the API.
  å–å†³äº APIã€‚

<!-- TODO mega snippet to enable all profilers -->

## CPU Profiler

## CPU åˆ†æå™¨

Go's CPU profiler can help you identify which parts of your code base consume a lot of CPU time.

Go çš„ CPU åˆ†æå™¨å¯ä»¥å¸®åŠ©æ‚¨æ‰¾å‡ºä»£ç ä¸­çš„å“ªäº›éƒ¨åˆ†æ¶ˆè€—å¤§é‡ CPU æ—¶é—´ã€‚

âš ï¸ Please note that CPU time is usually different from the real time experienced by your users (aka latency). For example a typical http request might take `100ms` to complete, but only consume `5ms` of CPU time while waiting for `95ms` on a database. It's also possible for a request to take `100ms`, but spend `200ms` of CPU if two goroutines are performing CPU intensive work in parallel. If this is confusing to you, please refer to the [Goroutine Scheduler](#goroutine-scheduler) section.

âš ï¸ è¯·æ³¨æ„ï¼ŒCPU æ—¶é—´é€šå¸¸ä¸åŒäºç”¨æˆ·å®é™…ä½“éªŒçš„æ—¶é—´ï¼ˆä¹Ÿç§°ä¸ºå»¶è¿Ÿï¼‰ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå…¸å‹çš„ http è¯·æ±‚å¯èƒ½éœ€è¦ 100 æ¯«ç§’æ‰èƒ½å®Œæˆï¼Œä½†åœ¨æ•°æ®åº“ä¸Šç­‰å¾… 95 æ¯«ç§’æ—¶åªæ¶ˆè€— 5 æ¯«ç§’çš„ CPU æ—¶é—´ã€‚å¦‚æœä¸¤ä¸ª goroutine å¹¶è¡Œæ‰§è¡Œ CPU å¯†é›†å‹å·¥ä½œï¼Œè¯·æ±‚ä¹Ÿå¯èƒ½éœ€è¦ 100 æ¯«ç§’ï¼Œä½†ä¼šèŠ±è´¹ 200 æ¯«ç§’çš„ CPUã€‚å¦‚æœè¿™è®©æ‚¨æ„Ÿåˆ°å›°æƒ‘ï¼Œè¯·å‚é˜… [Goroutine è°ƒåº¦å™¨](#goroutine-scheduler)éƒ¨åˆ†ã€‚

You can control the CPU profiler via various APIs:
æ‚¨å¯ä»¥é€šè¿‡å„ç§ API æ§åˆ¶ CPU åˆ†æå™¨ï¼š

- `go test -cpuprofile cpu.pprof` will run your tests and write a CPU profile to a file named `cpu.pprof`.
- [`pprof.StartCPUProfile(w)`](https://pkg.go.dev/runtime/pprof#StartCPUProfile) captures a CPU profile to `w` that covers the time span until [`pprof.StopCPUProfile()`](https://pkg.go.dev/runtime/pprof#StopCPUProfile) is called.
- [`import _ "net/http/pprof"`](https://pkg.go.dev/net/http/pprof) allows you to request a 30s CPU profile by hitting the `GET /debug/pprof/profile?seconds=30` endpoint of the default http server that you can start via `http.ListenAndServe("localhost:6060", nil)`.
- [`runtime.SetCPUProfileRate()`](https://pkg.go.dev/runtime#SetCPUProfileRate) lets you to control the sampling rate of the CPU profiler. See [CPU Profiler Limitations](#cpu-profiler-limitations) for current limitations.
- [`runtime.SetCgoTraceback()`](https://pkg.go.dev/runtime#SetCgoTraceback) can be used to get stack traces into cgo code. [benesch/cgosymbolizer](https://github.com/benesch/cgosymbolizer) has an implementation for Linux and macOS.

If you need a quick snippet to paste into your `main()` function, you can use the code below:

å¦‚æœä½ éœ€è¦ä¸€ä¸ªå¿«é€Ÿçš„ä»£ç ç‰‡æ®µæ¥ç²˜è´´åˆ°ä½ çš„ main() å‡½æ•°ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼š

```go
file, _ := os.Create("./cpu.pprof")
pprof.StartCPUProfile(file)
defer pprof.StopCPUProfile()
```

Regardless of how you activate the CPU profiler, the resulting profile will essentially be a table of stack traces formatted in the binary [pprof](../pprof.md) format. A simplified version of such a table is shown below:

æ— è®ºä½ å¦‚ä½•è§¦å‘ CPU åˆ†æå™¨ï¼Œç”Ÿæˆçš„profileæ–‡ä»¶æœ¬è´¨ä¸Šéƒ½æ˜¯ä»¥äºŒè¿›åˆ¶ [pprof](../pprof.md) æ ¼å¼æ ¼å¼åŒ–çš„å †æ ˆè·Ÿè¸ªè¡¨ã€‚è¿™ç§è¡¨æ ¼çš„ç®€åŒ–ç‰ˆæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š

|stack trace|samples/count|cpu/nanoseconds|
|-|-|-|
|main;foo|5|50000000|
|main;foo;bar|3|30000000|
|main;foobar|4|40000000|

The CPU profiler captures this data by asking the operating system to monitor the CPU usage of the application and sends it a `SIGPROF` signal for every `10ms` of CPU time it consumes. The OS also includes time consumed by the kernel on behalf of the application in this monitoring. Since the signal deliver rate depends on CPU consumption, it's dynamic and can be up to `N * 100Hz` where `N` is the number of logical CPU cores on the system. When a `SIGPROF` signal arrives, Go's signal handler captures a stack trace of the currently active goroutine, and increments the corresponding values in the profile. The `cpu/nanoseconds` value is currently directly derived from the sample count, so it is redundant, but convenient.

CPU åˆ†æå™¨é€šè¿‡æ“ä½œç³»ç»Ÿç›‘æ§åº”ç”¨ç¨‹åºçš„CPU ä½¿ç”¨æƒ…å†µï¼Œå¹¶ä¸”æ¯éš”`10ms`çš„CPU ç‰‡æ—¶é—´å‘é€ä¸€ä¸ª`SIGPROF`ä¿¡å·æ¥æ•è·profileæ•°æ®ã€‚æ“ä½œç³»ç»Ÿè¿˜åŒ…æ‹¬å†…æ ¸åœ¨æ­¤ç›‘æ§ä¸­ä»£è¡¨åº”ç”¨ç¨‹åºæ¶ˆè€—çš„æ—¶é—´ã€‚ç”±äºä¿¡å·ä¼ è¾“é€Ÿç‡å–å†³äº CPU æ¶ˆè€—ï¼Œå› æ­¤å®ƒæ˜¯åŠ¨æ€çš„ï¼Œæœ€é«˜å¯è¾¾ `N * 100Hz`ï¼Œå…¶ä¸­ `N` æ˜¯æ“ä½œç³»ç»Ÿä¸Šé€»è¾‘ CPU å†…æ ¸çš„æ•°é‡ã€‚å½“ `SIGPROF` ä¿¡å·åˆ°è¾¾æ—¶ï¼ŒGo çš„ä¿¡å·å¤„ç†ç¨‹åºæ•è·å½“å‰æ´»åŠ¨çš„ goroutine çš„å †æ ˆè·Ÿè¸ªï¼Œå¹¶å¢åŠ profileæ–‡ä»¶ä¸­çš„ç›¸åº”å€¼ã€‚ `cpu/nanoseconds` å€¼ç›®å‰æ˜¯ç›´æ¥ä»`samples/count`æ ·æœ¬è®¡æ•°ä¸­æ¨å¯¼å‡ºæ¥çš„ï¼Œæ‰€ä»¥æ˜¯å¤šä½™çš„ï¼Œä½†æ˜¯ä½¿ç”¨æ–¹ä¾¿ã€‚

### CPU Profiler Labels

### CPU åˆ†æå™¨æ ‡ç­¾

A cool feature of Go's CPU profiler is that you can attach arbitrary key value pairs to a goroutine. These labels will be inherited by any goroutine spawned from that goroutine and show up in the resulting profile.

Go çš„ CPU åˆ†æå™¨çš„ä¸€ä¸ªå¾ˆé…·çš„åŠŸèƒ½æ˜¯ä½ å¯ä»¥å°†ä»»æ„é”®å€¼å¯¹é™„åŠ åˆ° goroutineã€‚è¿™äº›æ ‡ç­¾å°†è¢«ä»è¯¥ goroutine ç”Ÿæˆçš„ä»»ä½• goroutine ç»§æ‰¿ï¼Œå¹¶å†™å…¥åˆ°profileæ–‡ä»¶ä¸­ã€‚

Let's consider the [example](./cpu-profiler-labels.go) below that does some CPU `work()` on behalf of a `user`. By using the [`pprof.Labels()`](https://pkg.go.dev/runtime/pprof#Labels) and [`pprof.Do()`](https://pkg.go.dev/runtime/pprof#Do) API, we can associate the `user` with the goroutine that is executing the `work()` function. Additionally the labels are automatically inherited by any goroutine spawned within the same code block, for example the `backgroundWork()` goroutine.

è®©æˆ‘ä»¬è€ƒè™‘ä¸‹é¢çš„ä¾‹å­ï¼Œå®ƒä»£è¡¨ç”¨æˆ·æ‰§è¡Œä¸€äº› CPU `work()`ã€‚é€šè¿‡ä½¿ç”¨ [pprof.Labels()](https://pkg.go.dev/runtime/pprof#Labels) å’Œ [pprof.Do()](https://pkg.go.dev/runtime/pprof#Do) APIï¼Œæˆ‘ä»¬å¯ä»¥å°†ç”¨æˆ·ä¸æ‰§è¡Œ work() å‡½æ•°çš„ goroutine ç›¸å…³è”ã€‚è¯¥æ ‡ç­¾ä¼šè¢«åŒä¸€ä»£ç å—ä¸­ç”Ÿæˆçš„ä»»ä½• goroutine è‡ªåŠ¨ç»§æ‰¿ï¼Œä¾‹å¦‚ `backgroundWork()`è¿™ä¸ªgoroutineã€‚

```go
func work(ctx context.Context, user string) {
	labels := pprof.Labels("user", user)
	pprof.Do(ctx, labels, func(_ context.Context) {
		go backgroundWork()
		directWork()
	})
}
```

The resulting profile will include a new label column and might look something like this:

ç”Ÿæˆçš„profileæ–‡ä»¶ä¸­å°†åŒ…å«ä¸€ä¸ªæ–°çš„æ ‡ç­¾åˆ—ï¼Œå¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

|stack trace|label|samples/count|cpu/nanoseconds|
|-|-|-|-|
|main.childWork|user:bob|5|50000000|
|main.childWork|user:alice|2|20000000|
|main.work;main.directWork|user:bob|4|40000000|
|main.work;main.directWork|user:alice|3|30000000|

Viewing the same profile with pprof's Graph view will also include the labels:
ä½¿ç”¨ pprof çš„ Graph è§†å›¾æŸ¥çœ‹ç›¸åŒçš„profileæ–‡ä»¶ä¹Ÿå°†åŒ…æ‹¬æ ‡ç­¾ï¼š

<img src="./cpu-profiler-labels.png" width=400/>

How you use these labels is up to you. You might include things such as `user ids`, `request ids`, `http endpoints`, `subscription plan` or other data that can allow you to get a better understanding of what types of requests are causing high CPU utilization, even when they are being processed by the same code paths. That being said, using labels will increase the size of your pprof files. So you should probably start with low cardinality labels such as endpoints before moving on to high cardinality labels once you feel confident that they don't impact the performance of your application.

å¦‚ä½•ä½¿ç”¨è¿™äº›æ ‡ç­¾å–å†³äºä½ ã€‚ä½ å¯èƒ½ä¼šåŒ…å«è¯¸å¦‚`user ids`ã€`request ids`ã€`http endpoints`ã€`subscription plan`æˆ–å…¶ä»–æ•°æ®ä¹‹ç±»çš„å†…å®¹ï¼Œè¿™äº›æ•°æ®å¯ä»¥è®©æ‚¨æ›´å¥½åœ°äº†è§£å“ªäº›ç±»å‹çš„è¯·æ±‚ä¼šå¯¼è‡´ CPU è´Ÿè½½é«˜ã€‚å³ä½¿å®ƒä»¬æ˜¯ç”±ç›¸åŒçš„ä»£ç å¤„ç†çš„è·¯å¾„ï¼Œè¯è™½å¦‚æ­¤ï¼Œä½¿ç”¨æ ‡ç­¾ä¼šå¢åŠ  pprof æ–‡ä»¶çš„å¤§å°ã€‚å› æ­¤ï¼Œä¸€æ—¦æ‚¨ç¡®ä¿¡å®ƒä»¬ä¸ä¼šå½±å“åº”ç”¨ç¨‹åºçš„æ€§èƒ½ï¼Œæ‚¨å¯èƒ½åº”è¯¥ä»ç±»ä¼¼`http endpoints`ç­‰ä½åŸºæ•°(low cardinality)æ ‡ç­¾å¼€å§‹ï¼Œç„¶åå†è½¬å‘é«˜åŸºæ•°(high cardinality)æ ‡ç­¾ã€‚

âš ï¸ Go 1.17 and below contained several bugs that could cause some profiler labels to be missing from CPU profiles, see [CPU Profiler Limitations](#cpu-profiler-limitations) for more information.

âš ï¸ Go 1.17 åŠæ›´ä½ç‰ˆæœ¬åŒ…å«å‡ ä¸ªå¯èƒ½å¯¼è‡´ CPU profileæ–‡ä»¶ä¸­ç¼ºå°‘æŸäº›åˆ†æå™¨æ ‡ç­¾çš„é”™è¯¯ï¼Œæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[CPU åˆ†æå™¨çš„é™åˆ¶](#cpu-profiler-limitations)ã€‚

### CPU Utilization

### CPU åˆ©ç”¨ç‡

Since the sample rate of the CPU profiler adapts to amount of CPU your program is consuming, you can derive the CPU utilization from CPU profiles. In fact pprof will do this automatically for you. For example the profile below was taking from a program that had an average CPU utilization of `147.77%`:

ç”±äº CPU åˆ†æå™¨çš„é‡‡æ ·ç‡ä¼šé€‚åº”æ‚¨çš„ç¨‹åºæ¶ˆè€—çš„ CPU çš„æ•°é‡ï¼Œå› æ­¤æ‚¨å¯ä»¥ä» CPU profileæ–‡ä»¶ä¸­å¾—å‡º CPU åˆ©ç”¨ç‡ã€‚äº‹å®ä¸Š pprof ä¼šè‡ªåŠ¨ä¸ºä½ åšè¿™ä»¶äº‹ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„profileæ–‡ä»¶æ˜¾ç¤ºç¨‹åºçš„å¹³å‡ CPU åˆ©ç”¨ç‡ä¸º `147.77%` ï¼š

```
$ go tool pprof guide/cpu-utilization.pprof
Type: cpu
Time: Sep 9, 2021 at 11:34pm (CEST)
Duration: 1.12s, Total samples = 1.65s (147.77%)
Entering interactive mode (type "help" for commands, "o" for options)
(pprof) 
```

Another popular way to express CPU utilization is CPU cores. In the example above the program was using an average of `1.47` CPU cores during the profiling period.

è¡¨ç¤º CPU åˆ©ç”¨ç‡çš„å¦ä¸€ç§æµè¡Œæ–¹å¼æ˜¯ CPU æ ¸æ•°ã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œç¨‹åºåœ¨åˆ†ææœŸé—´å¹³å‡ä½¿ç”¨äº† 1.47 ä¸ª CPU å†…æ ¸ã€‚

âš ï¸ In Go 1.17 and below you shouldn't put too much trust in this number if it's near or higher than `250%`, see [CPU Profiler Limitations](#cpu-profiler-limitations). However, if you see a very low number such as `10%` this usually indicates that CPU consumption is not an issue for your application. A common mistake is to ignore this number and start worrying about a particular function taking up a long time relative to the rest of the profile. This is usually a waste of time when overall CPU utilization is low, as not much can be gained from optimizing this function.

âš ï¸ åœ¨ Go 1.17 åŠä»¥ä¸‹ç‰ˆæœ¬ä¸­ï¼Œå¦‚æœå®ƒæ¥è¿‘æˆ–é«˜äº `250%`ï¼Œä½ ä¸è¦è¿‡äºç›¸ä¿¡è¿™ä¸ªæ•°å­—ï¼Œè¯·å‚é˜…[CPU åˆ†æå™¨çš„é™åˆ¶](#cpu-profiler-limitations)ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ‚¨çœ‹åˆ°ä¸€ä¸ªéå¸¸ä½çš„æ•°å­—ï¼ˆä¾‹å¦‚ 10%ï¼‰ï¼Œè¿™é€šå¸¸è¡¨æ˜ CPU æ¶ˆè€—å¯¹æ‚¨çš„åº”ç”¨ç¨‹åºæ¥è¯´ä¸æ˜¯é—®é¢˜ã€‚ä¸€ä¸ªå¸¸è§çš„é”™è¯¯æ˜¯å¿½ç•¥æ­¤æ•°å­—å¹¶å¼€å§‹æ‹…å¿ƒç‰¹å®šåŠŸèƒ½ç›¸å¯¹äºprofileçš„å…¶ä½™éƒ¨åˆ†ä¼šå ç”¨å¾ˆé•¿æ—¶é—´ã€‚å½“æ•´ä½“ CPU åˆ©ç”¨ç‡è¾ƒä½æ—¶ï¼Œè¿™é€šå¸¸æ˜¯æµªè´¹æ—¶é—´ï¼Œå› ä¸ºä¼˜åŒ–æ­¤åŠŸèƒ½å¹¶ä¸èƒ½è·å¾—å¤ªå¤šæ”¶ç›Šã€‚

### System Calls in CPU Profiles

### CPU profileæ–‡ä»¶ä¸­çš„ç³»ç»Ÿè°ƒç”¨

If you see system calls such as `syscall.Read()` or `syscall.Write()` using a lot of time in your CPU profiles, please note that this is only the CPU time spend inside of these functions in the kernel. The I/O time itself is not being tracked. Spending a lot of time on system calls is usually a sign of making too many of them, so perhaps increasing buffer sizes can help. For more complicated situations like this, you should consider using Linux perf, as it can also show you kernel stack traces that might provide you with additional clues.

å¦‚æœæ‚¨åœ¨ CPU profileæ–‡ä»¶ä¸­çœ‹åˆ°è¯¸å¦‚ `syscall.Read()` æˆ– `syscall.Write()` ä¹‹ç±»çš„ç³»ç»Ÿè°ƒç”¨ä½¿ç”¨å¤§é‡æ—¶é—´ï¼Œè¯·æ³¨æ„è¿™åªæ˜¯å†…æ ¸ä¸­è¿™äº›å‡½æ•°å†…éƒ¨èŠ±è´¹çš„ CPU æ—¶é—´ã€‚I/O æ—¶é—´æœ¬èº«æ²¡æœ‰è¢«è¿½è¸ªã€‚åœ¨ç³»ç»Ÿè°ƒç”¨ä¸ŠèŠ±è´¹å¤§é‡æ—¶é—´é€šå¸¸è¡¨æ˜è°ƒç”¨è¿‡å¤šï¼Œå› æ­¤å¢åŠ ç¼“å†²åŒºå¤§å°å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚å¯¹äºåƒè¿™æ ·æ›´å¤æ‚çš„æƒ…å†µï¼Œæ‚¨åº”è¯¥è€ƒè™‘ä½¿ç”¨ Linux perfï¼Œå› ä¸ºå®ƒè¿˜å¯ä»¥å‘æ‚¨æ˜¾ç¤ºå†…æ ¸å †æ ˆè·Ÿè¸ªï¼Œè¿™å¯èƒ½ä¼šä¸ºæ‚¨æä¾›é¢å¤–çš„çº¿ç´¢ã€‚

<!-- TODO: Write up some implementation details, e.g. mention setitimer(). -->
### CPU Profiler Limitations

### CPU åˆ†æå™¨çš„é™åˆ¶

There are a few known issues and limitations of the CPU profiler that you might want to be aware of:

æœ‰ä¸€äº›å·²çŸ¥çš„CPUåˆ†æå™¨çš„é—®é¢˜å’Œé™åˆ¶ï¼Œä½ åº”è¯¥çŸ¥é“å’Œäº†è§£ï¼š

- ğŸ [GH #35057](https://github.com/golang/go/issues/35057): CPU profiles taken with Go versions <= 1.17 become somewhat inaccurate for programs utilizing more than 2.5 CPU cores. Generally speaking the overall CPU utilization will be underreported, and workload spikes may be underrepresented in the resulting profile as well. This is fixed in Go 1.18. Meanwhile you could try to use Linux perf as a workaround.
  å¯¹äºä½¿ç”¨è¶…è¿‡ 2.5 ä¸ª CPU å†…æ ¸çš„ç¨‹åºï¼Œä½¿ç”¨ Go ç‰ˆæœ¬ <= 1.17 è·å–çš„ CPU profileæ–‡ä»¶å˜å¾—æœ‰äº›ä¸å‡†ç¡®ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ€»ä½“ CPU åˆ©ç”¨ç‡å°†è¢«ä½ä¼°ï¼Œå¹¶ä¸”å·¥ä½œè´Ÿè½½å³°å€¼ä¹Ÿå¯èƒ½åœ¨ç”Ÿæˆçš„profileæ–‡ä»¶ä¸­è¢«ä½ä¼°ã€‚è¿™åœ¨ Go 1.18 ä¸­å·²ä¿®å¤ã€‚åŒæ—¶ï¼Œæ‚¨å¯ä»¥å°è¯•ä½¿ç”¨ Linux perf ä½œä¸ºè§£å†³æ–¹æ³•ã€‚
- ğŸ Profiler labels in Go versions <= 1.17 suffered from several bugs.
  Go ç‰ˆæœ¬ <= 1.17 ä¸­çš„ åˆ†æå™¨ æ ‡ç­¾å­˜åœ¨å‡ ä¸ªé”™è¯¯ã€‚
  - [GH #48577](https://github.com/golang/go/issues/48577) and [CL 367200](https://go-review.googlesource.com/c/go/+/367200/): Labels were missing for goroutines executing on the system stack, executing C code, or making system calls.
    å½“goroutineæ‰§è¡Œåœ¨ç³»ç»Ÿæ ˆï¼Œæ‰§è¡ŒCä»£ç æˆ–è€…è¿›è¡Œç³»ç»Ÿè°ƒç”¨æ—¶å€™ï¼Œå°†ä¼šä¸¢å¤±åˆ†æå™¨æ ‡ç­¾ã€‚
  - [CL 369741](https://go-review.googlesource.com/c/go/+/369741): The first batch of samples in a CPU profile had an off-by-one error causing a misattribution of labels.
    CPU profileæ–‡ä»¶ä¸­çš„ç¬¬ä¸€æ‰¹æ ·æœ¬æœ‰ä¸€ä¸ªé”™è¯¯ï¼Œå¯¼è‡´æ ‡ç­¾åˆ†é…é”™è¯¯ã€‚
  - [CL 369983](https://go-review.googlesource.com/c/go/+/369983): System goroutines created on behalf of user goroutines (e.g. for garbage collection) incorrectly inherited their parents labels.
    ä»£è¡¨ç”¨æˆ·çº§ goroutineï¼ˆä¾‹å¦‚ç”¨äºåƒåœ¾æ”¶é›†ï¼‰åˆ›å»ºçš„ç³»ç»Ÿçº§ goroutine é”™è¯¯åœ°ç»§æ‰¿äº†å®ƒä»¬çš„çˆ¶æ ‡ç­¾ã€‚
- âš ï¸ï¸ You can call [`runtime.SetCPUProfileRate()`](https://pkg.go.dev/runtime#SetCPUProfileRate) to adjust the CPU profiler rate before calling `runtime.StartCPUProfile()`. This will print a warning saying `runtime: cannot set cpu profile rate until previous profile has finished`. However, it still works within the limitation of the bug mentioned above. This issue was [initially raised here](https://github.com/golang/go/issues/40094), and there is an [accepted proposal for improving the API](https://github.com/golang/go/issues/42502).
  æ‚¨å¯ä»¥åœ¨è°ƒç”¨ `runtime.StartCPUProfile()` ä¹‹å‰è°ƒç”¨ `runtime.SetCPUProfileRate()` æ¥è°ƒæ•´ CPU åˆ†æå™¨é€Ÿç‡ã€‚è¿™å°†æ‰“å°ä¸€æ¡è­¦å‘Šï¼š`runtime: cannot set cpu profile rate until previous profile has finished`ã€‚ä½†æ˜¯ï¼Œå®ƒä»ç„¶å¯ä»¥åœ¨ä¸Šè¿°é”™è¯¯çš„é™åˆ¶èŒƒå›´å†…å·¥ä½œã€‚æ­¤é—®é¢˜æœ€åˆæ˜¯åœ¨[æ­¤issue](https://github.com/golang/go/issues/40094)ä¸­æå‡ºçš„ï¼Œå¹¶ä¸”å·²æœ‰ä¸€ä¸ª[å·²æ¥å—çš„æ”¹è¿› API](https://github.com/golang/go/issues/42502)çš„ææ¡ˆã€‚
- âš ï¸ The maximum number of nested function calls that can be captured in stack traces by the CPU profiler is currently [`64`](https://sourcegraph.com/search?q=context:global+repo:github.com/golang/go+file:src/*+maxCPUProfStack+%3D&patternType=literal). If your program is using a lot of recursion or other patterns that lead to deep stack depths, your CPU profile will include stack traces that are truncated. This means you will miss parts of the call chain that led to the function that was active at the time the sample was taken.
  CPU åˆ†æå™¨å¯ä»¥åœ¨å †æ ˆè·Ÿè¸ªä¸­æ•è·çš„æœ€å¤§åµŒå¥—å‡½æ•°è°ƒç”¨æ•°ç›®å‰ä¸º [64](https://sourcegraph.com/search?q=context:global+repo:github.com/golang/go+file:src/*+maxCPUProfStack+%3D&patternType=literal)ã€‚å¦‚æœæ‚¨çš„ç¨‹åºä½¿ç”¨å¤§é‡é€’å½’(recursion)æˆ–å…¶ä»–å¯¼è‡´å †æ ˆæ·±åº¦æ›´æ·±çš„æ¨¡å¼ï¼Œæ‚¨çš„ CPU profileæ–‡ä»¶ä¸­çš„æ ˆè·Ÿè¸ªå°†ä¼šè¢«æˆªæ–­ã€‚è¿™æ„å‘³ç€æ‚¨å°†é”™è¿‡è°ƒç”¨é“¾çš„æŸäº›éƒ¨åˆ†ï¼Œè¿™äº›éƒ¨åˆ†æ˜¯åœ¨é‡‡æ ·æ—¶å¤„äºæ´»åŠ¨çŠ¶æ€çš„å‡½æ•°ã€‚

## Memory Profiler

## å†…å­˜åˆ†æå™¨

Go's memory profiler can help you identify which parts of your code base perform a lot of heap allocations, as well as how many of these allocations were still reachable during the last garbage collection. Because of this, the profile produced by the memory profiler is also often referred to as a heap profile.


Go çš„å†…å­˜åˆ†æå™¨å¯ä»¥å¸®åŠ©æ‚¨ç¡®å®šä»£ç ä¸­çš„å“ªäº›éƒ¨åˆ†æ‰§è¡Œäº†å¤§é‡çš„å †åˆ†é…ï¼Œä»¥åŠåœ¨ä¸Šæ¬¡åƒåœ¾å›æ”¶æœŸé—´è¿™äº›åˆ†é…çš„å†…å­˜ä¸­æœ‰å¤šå°‘ä»ç„¶å¯ä»¥è®¿é—®ã€‚å› æ­¤ï¼Œå†…å­˜åˆ†æå™¨ç”Ÿæˆçš„profileæ–‡ä»¶ä¹Ÿé€šå¸¸ç§°ä¸ºå † profileæ–‡ä»¶ã€‚

Heap memory management related activities are often responsible for up to 20-30% of CPU time consumed by Go processes. Additionally the elimination of heap allocations can have second order effects that speed up other parts of your code due to decreasing the amount of cache thrashing that occurs when the garbage collector has to scan the heap. This means that optimizing memory allocations can often have a better return on investment than optimizing CPU-bound code paths in your program.

ä¸å †å†…å­˜ç®¡ç†ç›¸å…³çš„æ´»åŠ¨é€šå¸¸å  Go è¿›ç¨‹æ¶ˆè€—çš„ CPU æ—¶é—´çš„ 20-30%ã€‚æ­¤å¤–ï¼Œç”±äºå‡å°‘äº†åƒåœ¾å›æ”¶å™¨å¿…é¡»æ‰«æå †æ—¶å‘ç”Ÿçš„ç¼“å­˜æŠ–åŠ¨é‡ï¼Œå› æ­¤æ¶ˆé™¤å †åˆ†é…å¯èƒ½ä¼šäº§ç”ŸäºŒé˜¶æ•ˆåº”(second order effects)ï¼Œä»è€ŒåŠ å¿«ä»£ç çš„å…¶ä»–éƒ¨åˆ†ã€‚è¿™æ„å‘³ç€ä¼˜åŒ–å†…å­˜åˆ†é…é€šå¸¸å¯ä»¥æ¯”ä¼˜åŒ–ç¨‹åºä¸­çš„ CPUå¯†é›†å‹(CPU-bound)çš„ä»£ç è·¯å¾„è·å¾—æ›´å¥½çš„æŠ•èµ„å›æŠ¥ã€‚

âš ï¸ The memory profiler does not show stack allocations as these are generally much cheaper than heap allocations. Please refer to the [Garbage Collector](#garbage-collector) section for more details.

å†…å­˜åˆ†æå™¨ä¸æ˜¾ç¤ºæ ˆåˆ†é…ï¼Œå› ä¸ºå®ƒä»¬æˆæœ¬é€šå¸¸æ¯”å †åˆ†é…ä¾¿å®œå¾—å¤šã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[åƒåœ¾å›æ”¶å™¨](#garbage-collector)éƒ¨åˆ†ã€‚

You can control the memory profiler via various APIs:
æ‚¨å¯ä»¥é€šè¿‡å„ç§ API æ§åˆ¶å†…å­˜åˆ†æå™¨ï¼š

- `go test -memprofile mem.pprof` will run your tests and write a memory profile to a file named `mem.pprof`.
- [`pprof.Lookup("allocs").WriteTo(w, 0)`](https://pkg.go.dev/runtime/pprof#Lookup) writes a memory profile that contains allocation events since the start of the process to `w`.
- [`import _ "net/http/pprof"`](https://pkg.go.dev/net/http/pprof) allows you to request a 30s memory profile by hitting the `GET /debug/pprof/allocs?seconds=30` endpoint of the default http server that you can start via `http.ListenAndServe("localhost:6060", nil)`. This is also called a delta profile internally.
- [`runtime.MemProfileRate`](https://pkg.go.dev/runtime#MemProfileRate) lets you to control the sampling rate of the memory profiler. See [Memory Profiler Limitations](#memory-profiler-limitations) for current limitations.

If you need a quick snippet to paste into your `main()` function, you can use the code below:

å¦‚æœä½ éœ€è¦ä¸€ä¸ªå¿«é€Ÿçš„ä»£ç ç‰‡æ®µæ¥ç²˜è´´åˆ°ä½ çš„ `main()` å‡½æ•°ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼š

```go
file, _ := os.Create("./mem.pprof")
defer pprof.Lookup("allocs").WriteTo(file, 0)
defer runtime.GC()
```

Regardless of how you activate the Memory profiler, the resulting profile will essentially be a table of stack traces formatted in the binary [pprof](../pprof.md) format. A simplified version of such a table is shown below:

æ— è®ºæ‚¨å¦‚ä½•å¯ç”¨å†…å­˜åˆ†æå™¨ï¼Œç”Ÿæˆçš„profileæ–‡ä»¶æœ¬è´¨ä¸Šéƒ½æ˜¯ä»¥äºŒè¿›åˆ¶ [pprof](../pprof.md) æ ¼å¼æ ¼å¼åŒ–çš„å †æ ˆè·Ÿè¸ªè¡¨ã€‚è¿™ç§è¡¨æ ¼çš„ç®€åŒ–ç‰ˆæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š

|stack trace|alloc_objects/count|alloc_space/bytes|inuse_objects/count|inuse_space/bytes|
|-|-|-|-|-|
|main;foo|5|120|2|48|
|main;foo;bar|3|768|0|0|
|main;foobar|4|512|1|128|

A memory profile contains two major pieces of information:

å†…å­˜profileæ–‡ä»¶åŒ…å«ä¸‹é¢ä¸¤ä¸ªä¸»è¦ä¿¡æ¯ï¼š

- `alloc_*`: The amount of allocations that your program has made since the start of the process (or profiling period for delta profiles).
  è‡ªè¿›ç¨‹å¼€å§‹ï¼ˆæˆ–å¢é‡profileæ–‡ä»¶çš„åˆ†æå‘¨æœŸï¼‰ä»¥æ¥ï¼Œæ‚¨çš„ç¨‹åºå·²è¿›è¡Œçš„åˆ†é…é‡ã€‚
- `inuse_*`: The amount of allocations that your program has made that were still reachable during the last GC.
  æ‚¨çš„ç¨‹åºåœ¨ä¸Šæ¬¡ GC æœŸé—´ä»å¯è®¿é—®çš„åˆ†é…é‡ã€‚

You can use this information for various purposes. For example you can use the `alloc_*` data to determine which code paths might be producing a lot of garbage for the GC to deal with, and looking at the `inuse_*` data over time can help you with investigating memory leaks or high memory usage by your program.

æ‚¨å¯ä»¥å°†è¿™äº›ä¿¡æ¯ç”¨äºå„ç§ç›®çš„ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `alloc_*` æ•°æ®æ¥ç¡®å®šå“ªäº›ä»£ç è·¯å¾„å¯èƒ½ä¼šäº§ç”Ÿå¤§é‡åƒåœ¾ä¾› GC å¤„ç†ï¼Œå¹¶ä¸”éšç€æ—¶é—´çš„æ¨ç§»æŸ¥çœ‹ `inuse_*` æ•°æ®å¯ä»¥å¸®åŠ©æ‚¨åˆ†æç¨‹åºä¸­å†…å­˜æ³„æ¼æˆ–é«˜å†…å­˜ä½¿ç”¨æƒ…å†µã€‚

<!-- TODO: mention profiles are up to two gcs old -->
### Allocs vs Heap Profile

The [`pprof.Lookup()`](https://pkg.go.dev/runtime/pprof#Lookup) function as well as [net/http/pprof](https://pkg.go.dev/net/http/pprof) package expose the memory profile under two names: `allocs` and `heap`. Both profiles contain the same data, the only difference is that the `allocs` profile has `alloc_space/bytes` set as the default sample type, whereas the `heap` profile defaults to `inuse_space/bytes`. This is used by the pprof tool to decide which sample type to show by default.

[`pprof.Lookup()`](https://pkg.go.dev/runtime/pprof#Lookup) å‡½æ•°ä»¥åŠ [net/http/pprof](https://pkg.go.dev/net/http/pprof) åŒ…ä»¥ä¸¤ä¸ªåç§°å…¬å¼€å†…å­˜profileæ–‡ä»¶ï¼š`allocs` å’Œ `heap`ã€‚ä¸¤ä¸ªprofileæ–‡ä»¶åŒ…å«ç›¸åŒçš„æ•°æ®ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ `allocs` profileæ–‡ä»¶å°† `alloc_space/bytes` è®¾ç½®ä¸ºé»˜è®¤æ ·æœ¬ç±»å‹ï¼Œè€Œ`heap` profileæ–‡ä»¶é»˜è®¤ä¸º `inuse_space/bytes`ã€‚ pprof å·¥å…·ä½¿ç”¨å®ƒæ¥å†³å®šé»˜è®¤æ˜¾ç¤ºå“ªç§æ ·æœ¬ç±»å‹ã€‚

### Memory Profiler Sampling

### å†…å­˜åˆ†æå™¨é‡‡æ ·

To keep overhead low, the memory profiler uses poisson sampling so that on average only one allocation every `512KiB`
 triggers a stack trace to be taken and added to the profile. However, before the profile is written into the final pprof file, the runtime scales the collected sample values by dividing them through the sampling probability. This means that the amount of reported allocations should be a good estimate of the actual amount of allocations, regardless of the [`runtime.MemProfileRate`](https://pkg.go.dev/runtime#MemProfileRate) you are using.

 ä¸ºäº†ä¿æŒè¾ƒä½çš„å¼€é”€ï¼Œå†…å­˜åˆ†æå™¨ä½¿ç”¨æ³Šæ¾é‡‡æ ·(poisson sampling)ï¼Œå› æ­¤å¹³å‡æ¯ `512KiB` åªæœ‰ä¸€ä¸ªåˆ†é…è§¦å‘å †æ ˆè·Ÿè¸ªä»¥è¢«è·å–å¹¶æ·»åŠ åˆ°profileæ–‡ä»¶ä¸­ã€‚ä½†æ˜¯ï¼Œåœ¨å°†profileæ–‡ä»¶å†™å…¥æœ€ç»ˆ pprof æ–‡ä»¶ä¹‹å‰ï¼Œè¿è¡Œæ—¶é€šè¿‡å°†æ”¶é›†çš„æ ·æœ¬å€¼é™¤ä»¥é‡‡æ ·æ¦‚ç‡æ¥ç¼©æ”¾å®ƒä»¬ã€‚è¿™æ„å‘³ç€æŠ¥å‘Šçš„åˆ†é…é‡åº”è¯¥æ˜¯å¯¹å®é™…åˆ†é…é‡çš„è‰¯å¥½ä¼°è®¡ï¼Œæ— è®ºæ‚¨ä½¿ç”¨çš„æ˜¯ä»€ä¹ˆ [`runtime.MemProfileRate`](https://pkg.go.dev/runtime#MemProfileRate)ã€‚

For profiling in production, you should generally not have to modify the sampling rate. The only reason for doing so is if you're worried that not enough samples are getting collected in situations where very few allocations are taking place.

å¯¹äºç”Ÿäº§ä¸­çš„å†…å­˜åˆ†æï¼Œæ‚¨é€šå¸¸ä¸å¿…ä¿®æ”¹é‡‡æ ·ç‡ã€‚è¿™æ ·åšçš„å”¯ä¸€ç†ç”±æ˜¯ï¼Œå¦‚æœæ‚¨æ‹…å¿ƒåœ¨å†…å­˜åˆ†é…å¾ˆå°‘çš„æƒ…å†µä¸‹æ”¶é›†åˆ°çš„æ ·æœ¬ä¸è¶³ã€‚

### Memory Inuse vs RSS

### å†…å­˜ä½¿ç”¨ä¸ RSS

A common confusion is looking at the total amount of memory reported by the `inuse_space/bytes` sample type, and noticing that it doesn't match up with the RSS memory usage reported by the operating system. There are various reasons for this:

- RSS includes a lot more than just Go heap memory usage by definition, e.g. the memory used by goroutine stacks, the program executable, shared libraries as well as memory allocated by C functions.
- The GC may decide to not return free memory to the OS immediately, but this should be a lesser issue after [runtime changes in Go 1.16](https://golang.org/doc/go1.16#runtime).
- Go uses a non-moving GC, so in some cases free heap memory might be fragmented in ways that prevent Go from releasing it to the OS.


### Memory Profiler Implementation

The pseudo code below should capture the essential aspects of the memory profiler's implementation to give you a better intuition for it. As you can see, the `malloc()` function inside of the Go runtime uses `poisson_sample(size)` to determine if an allocation should be sampled. If yes, a stack trace `s` is taken and used as the key in the `mem_profile` hash map to increment the `allocs` and `alloc_bytes` counters. Additionally the `track_profiled(object, s)` call marks the `object` as a sampled allocation on the heap and associates the stack trace `s` with it.

```
func malloc(size):
  object = ... // allocation magic

  if poisson_sample(size):
    s = stacktrace()
    mem_profile[s].allocs++
    mem_profile[s].alloc_bytes += size
    track_profiled(object, s)

  return object
```

When the GC determines that it is time to free an allocated object, it calls `sweep()` which uses `is_profiled(object)` to check if the `object` is marked as a sampled object. If yes, it retrieves the stack trace `s` that lead to the allocation, and increments the `frees` and `free_bytes` counters for it inside of the `mem_profile`.

```
func sweep(object):
  if is_profiled(object)
    s = alloc_stacktrace(object)
    mem_profile[s].frees++
    mem_profile[s].free_bytes += sizeof(object)

	// deallocation magic
```

The `free_*` counters themselves are not included in the final memory profile. Instead they are used to calculate the `inuse_*` counters in the profile via simple `allocs - frees` subtraction. Additionally the final output values are scaled by dividing them through their sampling probability.

### Memory Profiler Limitations

There are a few known issues and limitations of the memory profiler that you might want to be aware of:

- ğŸ [GH #49171](https://github.com/golang/go/issues/49171): Delta profiles (taken with e.g. `GET /debug/pprof/allocs?seconds=60`) may report negative allocation counts due to a symbolization bug involving inlined closures in Go 1.17. It's fixed in Go 1.18.
- âš ï¸ [`runtime.MemProfileRate`](https://pkg.go.dev/runtime#MemProfileRate) must only be modified once, as early as possible in the startup of the program; for example, at the beginning of `main()`. Writing this value is inherently a small data race, and changing it multiple times during program execution will produce incorrect profiles.
- âš  When debugging potential memory leaks, the memory profiler can show you where those allocations were created, but it can't show you which references are keeping them alive. A few attempts to solve this problem were made over the years, but none of them work with recent versions of Go. If you know about a working tool, please [let me know](https://github.com/DataDog/go-profiler-notes/issues).
- âš  [CPU Profiler Labels](#cpu-profiler-labels) or similar are not supported by the memory profiler. It's difficult to add this feature to the current implementation as it could create a memory leak in the internal hash map that holds the memory profiling data.
- âš  Allocations made by cgo C code don't show up in the memory profile.
- âš  Memory profile data may be up to two garbage collection cycles old. If you want a more consistent point-in-time snapshot, consider calling `runtime.GC()` before requesting a memory profile. [net/http/pprof](https://pkg.go.dev/net/http/pprof) accepts a `?gc=1` argument for this purpose. For more information see the [runtime.MemProfile()](https://pkg.go.dev/runtime#MemProfile) docs, as well as as the comment on `memRecord` in [`mprof.go`](https://github.com/golang/go/blob/master/src/runtime/mprof.go).
- âš ï¸ The maximum number of nested function calls that can be captured in stack traces by the memory profiler is currently [`32`](https://sourcegraph.com/search?q=context:global+repo:github.com/golang/go+file:src/*+maxStack+%3D&patternType=literal), see [CPU Profiler Limitations](#cpu-profiler-limitations) for more information on what happens when you exceed this limit.
- âš ï¸ There is no size limit for the internal hash map that holds the memory profile. This means it will grow in size until it covers all allocating code paths in your code base. This is not a problem in practice, but might look like a small memory leak if you're observing the memory usage of your process.

## Block Profiler

The block profiler in Go measures how much time your goroutines spend Off-CPU while waiting for channel as well as mutex operations provided by the [sync](https://pkg.go.dev/sync) package. The following Go operations are hooked up to the block profiler:

- [select](https://github.com/golang/go/blob/go1.15.7/src/runtime/select.go#L511)
- [chan send](https://github.com/golang/go/blob/go1.15.7/src/runtime/chan.go#L279)
- [chan receive](https://github.com/golang/go/blob/go1.15.7/src/runtime/chan.go#L586)
- [semacquire](https://github.com/golang/go/blob/go1.15.7/src/runtime/sema.go#L150) ( [`Mutex.Lock`](https://golang.org/pkg/sync/#Mutex.Lock), [`RWMutex.RLock`](https://golang.org/pkg/sync/#RWMutex.RLock) , [`RWMutex.Lock`](https://golang.org/pkg/sync/#RWMutex.Lock), [`WaitGroup.Wait`](https://golang.org/pkg/sync/#WaitGroup.Wait))
- [notifyListWait](https://github.com/golang/go/blob/go1.15.7/src/runtime/sema.go#L515) ( [`Cond.Wait`](https://golang.org/pkg/sync/#Cond.Wait))

âš ï¸ Block profiles do not include time spend waiting on I/O, Sleep, GC and various other waiting states. Additionally blocking events are not recorded until they have completed, so the block profile can't be used to debug why a Go program is currently hanging. The latter can be determined using the Goroutine Profiler.

You can control the block profiler via various APIs:

- `go test -blockprofile block.pprof` will run your tests and write a block profile that captures every blocking event to a file named `block.pprof`.
- [`runtime.SetBlockProfileRate(rate)`](https://pkg.go.dev/runtime#SetBlockProfileRate) lets you to enable and control the sampling rate of the block profiler.
- [`pprof.Lookup("block").WriteTo(w, 0)`](https://pkg.go.dev/runtime/pprof#Lookup) writes a block profile that contains blocking events since the start of the process to `w`.
- [`import _ "net/http/pprof"`](https://pkg.go.dev/net/http/pprof) allows you to request a 30s block profile by hitting the `GET /debug/pprof/block?seconds=30` endpoint of the default http server that you can start via `http.ListenAndServe("localhost:6060", nil)`. This is also called a delta profile internally.


If you need a quick snippet to paste into your `main()` function, you can use the code below:

```go
runtime.SetBlockProfileRate(100_000_000) // WARNING: Can cause some CPU overhead
file, _ := os.Create("./block.pprof")
defer pprof.Lookup("block").WriteTo(file, 0)
```

Regardless of how you activate the block profiler, the resulting profile will essentially be a table of stack traces formatted in the binary [pprof](../pprof.md) format. A simplified version of such a table is shown below:

|stack trace|contentions/count|delay/nanoseconds|
|-|-|-|
|main;foo;runtime.selectgo|5|867549417|
|main;foo;bar;sync.(*Mutex).Lock|3|453510869|
|main;foobar;runtime.chanrecv1|4|5351086|

### Block Profiler Implementation

The pseudo code below should capture the essential aspects of the block profiler's implementation to give you a better intuition for it. When sending a message to channel, i.e. `ch <- msg`, Go invokes the `chansend()` function in the runtime that is shown below. If the channel is `ready()` to receive the message, the `send()` happens immediately. Otherwise the block profiler captures the `start` time of the blocking event and uses `wait_until_ready()` to ask the scheduler to move the goroutine off the CPU until the channel is ready. Once the channel is ready, the blocking `duration` is determined and used by `random_sample()` along with the sampling `rate` to decide if this block event should be recorded. If yes, the current stack trace `s` is captured and used as a key inside of the `block_profile` hash map to increment the `count` and `delay` values. After that the actual `send()` operation proceeds.

```
func chansend(channel, msg):
  if ready(channel):
    send(channel, msg)
    return

  start = now()
  wait_until_ready(channel) // Off-CPU Wait
  duration = now() - start

  if random_sample(duration, rate):
    s = stacktrace()
    // note: actual implementation is a bit trickier to correct for bias
    block_profile[s].contentions += 1
    block_profile[s].delay += duration

  send(channel, msg)
```

The `random_sample` function looks like shown below. If the block profiler is enabled, all events where `duration >= rate` are captured, and shorter events have a `duration/rate` chance of being captured.

```
func random_sample(duration, rate):
  if rate <= 0 || (duration < rate && duration/rate > rand(0, 1)):
    return false
  return true
```

In other words, if you set `rate` to `10.000` (the unit is nanoseconds), all blocking events lasting `10 Âµsec` or longer are captured. Additionally `10%` of events lasting `1 Âµsec` and `1%` of events lasting `100 nanoseconds`, and so on, are captured as well.

### Block vs Mutex Profiler

Both block and mutex profiler report time waiting on mutexes. The difference is that the block profiler captures the time waiting to acquire a `Lock()`, whereas the mutex profiler captures the time another goroutine was waiting before `Unlock()` allowed it to proceed.

In other words, the block profiler shows you which goroutines are experiencing increased latency due to mutex contentions whereas the mutex profiler shows you the goroutines that are holding the locks that are causing the contention.

### Block Profiler Limitations

- ğŸš¨ The block profiler can cause significant CPU overhead in production, so it's recommended to only use it for development and testing. If you do need to use it in production, start out with a very high rate, perhaps 100 million, and lower it only if needed. In the past this guide recommended a rate of `10,000` as safe, but we saw production workloads suffering up to 4% overhead under this setting, and even rates up to 10 million were not sufficient to significantly reduce the overhead.
- âš  Block profiles cover only a small subset of [Off-CPU waiting states](https://github.com/golang/go/blob/go1.17.1/src/runtime/runtime2.go#L1053-L1081) a goroutine can enter.
- âš ï¸ The maximum number of nested function calls that can be captured in stack traces by the memory profiler is currently [`32`](https://sourcegraph.com/search?q=context:global+repo:github.com/golang/go+file:src/*+maxStack+%3D&patternType=literal), see [CPU Profiler Limitations](#cpu-profiler-limitations) for more information on what happens when you exceed this limit.
- âš ï¸ There is no size limit for the internal hash map that holds the block profile. This means it will grow in size until it covers all blocking code paths in your code base. This is not a problem in practice, but might look like a small memory leak if you're observing the memory usage of your process.
- âš  [CPU Profiler Labels](#cpu-profiler-labels) or similar are not supported by the block profiler. It's difficult to add this feature to the current implementation as it could create a memory leak in the internal hash map that holds the memory profiling data.
- ğŸ Go 1.17 fixed a long-standing [sampling bias bug in the block profiler](../block-bias.md). Older versions of Go will overreport the impact of infrequent long blocking events over frequent short events.

## Mutex profiler

The mutex profiler measures how long goroutines spend blocking other goroutines. In other words, it measures the sources of lock contention. The mutex profiler can capture contention coming from `sync.Mutex` and `sync.RWMutex`.

âš ï¸ Mutex profiles do not include other sources of contention such as `sync.WaitGroup`, `sync.Cond`, or accessing file descriptors. Additionally, mutex contention is not recorded until the mutex is unlocked, so the mutex profile can't be used to debug why a Go program is currently hanging. The latter can be determined using the Goroutine Profiler.

You can control the mutex profiler via various APIs:

- `go test -mutexprofile mutex.pprof` will run your tests and write a mutex profile to a file named `mutex.pprof`.
- [`runtime.SetMutexProfileRate(rate)`](https://pkg.go.dev/runtime#SetMutexProfileRate) lets you to enable and control the sampling rate of the mutex profiler. If you set a sampling rate of `R`, then an average of `1/R` mutex contention events are captured. If the rate is 0 or less, nothing is captured.
- [`pprof.Lookup("mutex").WriteTo(w, 0)`](https://pkg.go.dev/runtime/pprof#Lookup) writes a mutex profile that contains mutex events since the start of the process to `w`.
- [`import _ "net/http/pprof"`](https://pkg.go.dev/net/http/pprof) allows you to request a 30s mutex profile by hitting the `GET /debug/pprof/mutex?seconds=30` endpoint of the default http server that you can start via `http.ListenAndServe("localhost:6060", nil)`. This is also called a delta profile internally.


If you need a quick snippet to paste into your `main()` function, you can use the code below:

```go
runtime.SetMutexProfileFraction(100)
file, _ := os.Create("./mutex.pprof")
defer pprof.Lookup("mutex").WriteTo(file, 0)
```

The resulting mutex profile will essentially be a table of stack traces formatted in the binary [pprof](../pprof.md) format. A simplified version of such a table is shown below:

|stack trace|contentions/count|delay/nanoseconds|
|-|-|-|
|main;foo;sync.(*Mutex).Unlock|5|867549417|
|main;bar;baz;sync.(*Mutex).Unlock|3|453510869|
|main;foobar;sync.(*RWMutex).RUnlock|4|5351086|

âš ï¸ See the section on [block vs mutex profiles](#block-vs-mutex-profiler) for the difference between the two profiles.

### Mutex profiler implementation

The mutex profiler is implemented by recording the time from when a goroutine tries to acquire a lock (e.g. `mu.Lock()`) to when the lock is released by the goroutine holding the lock (e.g. `mu.Unlock()`). First, a goroutine calls `semacquire()` to take the lock, and records the time it started waiting if the lock is already held. When the goroutine holding the lock releases it by calling `semrelease()`, the goroutine will look for the next goroutine waiting for the lock, and see how long that goroutine spent waiting. The current mutex profiling value `rate` is used to randomly decide whether to record this event. If it's randomly chosen, the blocking time is recorded to a `mutex_profile` hash map keyed by the call stack where the goroutine released the lock.

In pseudocode:

```
func semacquire(lock):
  if lock.take():
    return

  start = now()
  waiters[lock].add(this_goroutine(), start)
  wait_for_wake_up()

func semrelease(lock):
  next_goroutine, start = waiters[lock].get()
  if !next_goroutine:
    // If there weren't any waiting goroutines, there is no contention to record
    return

  duration = now() - start
  if rand(0,1) < 1 / rate:
    s = stacktrace()
    mutex_profile[s].contentions += 1
    mutex_profile[s].delay += duration

  wake_up(next_goroutine)
```

### Mutex Profiler Limitations

The mutex profiler has limitations similar to the block profiler:

- âš ï¸ The maximum number of nested function calls that can be captured in stack traces by the mutex profiler is currently [`32`](https://sourcegraph.com/search?q=context:global+repo:github.com/golang/go+file:src/*+maxStack+%3D&patternType=literal), see [CPU Profiler Limitations](#cpu-profiler-limitations) for more information on what happens when you exceed this limit.
- âš ï¸ There is no size limit for the internal hash map that holds the mutex profile. This means it will grow in size until it covers all blocking code paths in your code base. This is not a problem in practice, but might look like a small memory leak if you're observing the memory usage of your process.
- âš  [CPU Profiler Labels](#cpu-profiler-labels) or similar are not supported by mutex profiler. It's difficult to add this feature to the current implementation as it could create a memory leak in the internal hash map that holds the memory profiling data.
- âš ï¸ The contention counts and delay times in a mutex profile are adjusted at reporting time based on the *most recent* configured sampling rate, rather than at sample time. As a result, programs which change the mutex profile fraction in the middle of execution can see skewed counts and delays.

## Goroutine Profiler

This profiler is currently documented in a separate document, see [goroutine.md](../goroutine.md). It will be integrated into this document soon.

## ThreadCreate Profiler

ğŸ The threadcreate profile is intended to show stack traces that led to the creation of new OS threads. However, it's been [broken since 2013](https://github.com/golang/go/issues/6104), so you should stay away from it.

# Advanced Topics

## Stack Traces

This is currently documented in a separate document, see [stack-traces.md](../stack-traces.md). It will be integrated into this document soon.
## pprof Format

This is currently documented in a separate document, see [pprof.md](../pprof.md). It will be integrated into this document soon.

# Disclaimers

I'm [felixge](https://github.com/felixge) and work at [Datadog](https://www.datadoghq.com/) on [Continuous Profiling](https://www.datadoghq.com/product/code-profiling/) for Go. You should check it out. We're also [hiring](https://www.datadoghq.com/jobs-engineering/#all&all_locations) : ).

The information on this page is believed to be correct, but no warranty is provided. Feedback is welcome!

Credits:
- [Nick Ripley](https://github.com/nsrip-dd) for contributing the [Mutex Profiler](#mutex-profiler) section.

<!--
Notes:

- Heap: Maybe a good article to link: https://medium.com/@ankur_anand/a-visual-guide-to-golang-memory-allocator-from-ground-up-e132258453ed
- GC Cost: O(N) with regards to live allocations on the heap containing pointers.
- Each pointer slot in an allocation has a cost! Even nil pointers.
- Reducing Costs: Talk about CPU, Memory and Networking. Is it possible to profile the latter?
- pprof: Maybe host a service to convert perf.data files into pprof files?
- Reuse cute gophers from conf talks.
- pprof cli tips from rhys h. on gopher slack: Favorite options include edgefraction=0, nodefraction=0, and nodecount of something larger than 80 (but rendering gets slow). Plus focus, and an ever-growing regexp (as I dive in to the profile) in ignore.
- https://profiler.firefox.com/ can view linux perf files? With time axis? see https://www.markhansen.co.nz/profiler-uis/

-->
